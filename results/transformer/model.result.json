{
    "name": "Transformer",
    "arguments": {
        "dims": 48,
        "q_heads": 8,
        "kv_heads": 8,
        "blocks": 8,
        "base_freq": 10000
    },
    "variation": null,
    "title": "Transformer",
    "description": "Decoder stack from 'Attention Is All You Need'.",
    "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N. Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
    ],
    "paper": "https://arxiv.org/abs/1706.03762",
    "metrics": {
        "Memory Capacity": 0.6191981991061337,
        "State Management": 0.07430168739050924
    },
    "model_stats": {
        "size": 4002560,
        "size_doubling_rate": 0.0,
        "size_big_o": "???",
        "flops": 36455333888,
        "flops_doubling_rate": 1.4299999475479126,
        "flops_big_o": "???"
    }
}